{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Let's create a simple neural network that will classify the Iris flower dataset. The following is the code block for creating a simple neural network: "
      ],
      "metadata": {
        "id": "QgyxAMQ25SsF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LGwJJgnY5FJ-"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "dataset = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data', names=['sepal_length', 'sepal_width', 'petal_length', 'petal_width', 'species'])\n",
        "\n",
        "dataset['species'] = pd.Categorical(dataset['species']).codes\n",
        "\n",
        "dataset = dataset.sample(frac=1, random_state=1234)\n",
        "\n",
        "# split the data set into train and test subsets\n",
        "train_input = dataset.values[:120, :4]\n",
        "train_target = dataset.values[:120, 4]\n",
        "\n",
        "test_input = dataset.values[120:, :4]\n",
        "test_target = dataset.values[120:, 4]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The preceding code is boilerplate code that downloads the Iris dataset CSV file and then loads it into the pandas DataFrame. We then shuffle the DataFrame rows and split the code into numpy arrays, train_input/train_target (flower properties/flower class), for the training data and test_input/test_target for the test data. \n",
        "We'll use 120 samples for training and 30 for testing. If you are not familiar with pandas, think of this as an advanced version of NumPy."
      ],
      "metadata": {
        "id": "qhvYzopj5Sl4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's define ourfirst neural network. We'll use a feedforward network with one hidden layer with five units, a ReLU activation function (this is just another type of activation, defined simply as *f(x) = max(0, x)*), and an output layer with three units. The output layer has three units, whereas each unit corresponds to one of the three classes of Iris flower. The following is the PyTorch definition of the network:"
      ],
      "metadata": {
        "id": "pccpZgHU8iAB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "torch.manual_seed(1234)\n",
        "\n",
        "hidden_units = 5\n",
        "\n",
        "net = torch.nn.Sequential(\n",
        "    torch.nn.Linear(4, hidden_units), # we'll use a network with 4 hidden units\n",
        "    torch.nn.ReLU(), # ReLU activation\n",
        "    torch.nn.Linear(hidden_units, 3) # 3 output units for each of the 3 possible classes\n",
        ")"
      ],
      "metadata": {
        "id": "_dOD1OMg8knA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll use one-hot encoding for the target data. This means that each class of the flower will be represented as an array (Iris Setosa = [1, 0, 0], Iris Versicolour = [0, 1, 0], and Iris Virginica = [0, 0, 1]), and one element of the array will be the target for one unit of the output layer. When the network classifies a new sample, we'll determine the class by taking the unit with the highest activation value. \n",
        "`torch.manual_seed(1234)` enables us to use the same random data every time for the reproducibility of results. "
      ],
      "metadata": {
        "id": "ri9tmJd_87qB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we'll choose the loss function:"
      ],
      "metadata": {
        "id": "4iMaXLyq-ES2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = torch.nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "SiqLpoz6-Ic9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "With the `criterion` variable, we define the loss function that we'll use, in this case, this is cross-entropy loss. The loss function will measure how different the output of the network is compared to the target data."
      ],
      "metadata": {
        "id": "cv7_EwIu-SGs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We then define the stochastic gradient descent (SGD) optimizer (a variation of the gradient descent algorithm) with a learning rate of 0.1 and a momentum of 0.9:"
      ],
      "metadata": {
        "id": "qFxHfeDl-ZLC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.SGD(net.parameters(), lr=0.1, momentum=0.9)"
      ],
      "metadata": {
        "id": "iCGhvKK0-5DT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let's train the network: "
      ],
      "metadata": {
        "id": "RV5Wiw3k_FDI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 50\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    inputs = torch.autograd.Variable(torch.Tensor(train_input).float())\n",
        "    targets = torch.autograd.Variable(torch.Tensor(train_target).long())\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    out = net(inputs)\n",
        "    loss = criterion(out, targets)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if epoch == 0 or (epoch + 1) % 10 == 0:\n",
        "        print('Epoch %d Loss: %.4f' % (epoch + 1, loss.item()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sRkD7513_I9i",
        "outputId": "ca3ee637-fc47-4e44-ceea-6791b95ca248"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 Loss: 1.2181\n",
            "Epoch 10 Loss: 0.6745\n",
            "Epoch 20 Loss: 0.2447\n",
            "Epoch 30 Loss: 0.1397\n",
            "Epoch 40 Loss: 0.1001\n",
            "Epoch 50 Loss: 0.0855\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll run the training for 50 epochs, which means that we'll iterate 50 times over the training dataset: \n",
        "\n",
        "\n",
        "1.   Create the torch variable that are `input` and `target` from the numpy array train_input and train_target. \n",
        "2.   Zero the gradients of the optimizer to prevent accumulation from the previous iterations. We feed the training data to the neural network net (input) and we compute the loss function criterion (out, targets) between the network output and the target data.\n",
        "3.   Propagate the loss value back through the network. We do this so that we can calculate how each network weight affects the loss function. \n",
        "4.   The optimizer updates the weights of the network in a way that will reduce the future loss function values."
      ],
      "metadata": {
        "id": "oqVJH4UU_SH-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's see what the final accuracy of our model is: "
      ],
      "metadata": {
        "id": "kVfDIxE6AJpC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "inputs = torch.autograd.Variable(torch.Tensor(test_input).float())\n",
        "targets = torch.autograd.Variable(torch.Tensor(test_target).long())\n",
        "\n",
        "optimizer.zero_grad()\n",
        "out = net(inputs)\n",
        "_, predicted = torch.max(out.data, 1)\n",
        "\n",
        "error_count = test_target.size - np.count_nonzero((targets == predicted).numpy())\n",
        "print('Errors: %d; Accuracy: %d%%' % (error_count, 100 * torch.sum(targets == predicted) / test_target.size))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "13bt21qAALGI",
        "outputId": "012148f3-d2ba-443f-fe17-a41d71c77661"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Errors: 0; Accuracy: 100%\n"
          ]
        }
      ]
    }
  ]
}