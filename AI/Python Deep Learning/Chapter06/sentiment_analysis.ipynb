{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "V100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "In this example, we'll use PyTorch to implement a LSTM-based sentiment analysis over the [Large Movie Review Dataset](http://ai.stanford.edu/~amaas/data/sentiment/). The model will take as input a text sequence, which represents a movie review and will output a binary result of whether the review is positive or negative.\n",
        "\n",
        "_This example is partially based on_ [https://github.com/bentrevett/pytorch-sentiment-analysis](https://github.com/bentrevett/pytorch-sentiment-analysis)\n"
      ],
      "metadata": {
        "id": "SN2FhXmtiand"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch\n",
        "!pip install torchtext\n",
        "!pip install portalocker"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dzJm64Lxijd9",
        "outputId": "a8c1eb08-5eff-414a-ff6c-c77aa216dd68"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.0.1+cu118)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.12.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (3.27.4.1)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (16.0.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Requirement already satisfied: torchtext in /usr/local/lib/python3.10/dist-packages (0.15.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torchtext) (4.66.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchtext) (2.31.0)\n",
            "Requirement already satisfied: torch==2.0.1 in /usr/local/lib/python3.10/dist-packages (from torchtext) (2.0.1+cu118)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchtext) (1.23.5)\n",
            "Requirement already satisfied: torchdata==0.6.1 in /usr/local/lib/python3.10/dist-packages (from torchtext) (0.6.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchtext) (3.12.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchtext) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchtext) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchtext) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchtext) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.0.1->torchtext) (2.0.0)\n",
            "Requirement already satisfied: urllib3>=1.25 in /usr/local/lib/python3.10/dist-packages (from torchdata==0.6.1->torchtext) (2.0.4)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch==2.0.1->torchtext) (3.27.4.1)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch==2.0.1->torchtext) (16.0.6)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext) (2023.7.22)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.0.1->torchtext) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.0.1->torchtext) (1.3.0)\n",
            "Collecting portalocker\n",
            "  Downloading portalocker-2.7.0-py2.py3-none-any.whl (15 kB)\n",
            "Installing collected packages: portalocker\n",
            "Successfully installed portalocker-2.7.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define the device (GPU by default with a fallback on CPU):"
      ],
      "metadata": {
        "id": "AM1iB5a8i4tP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "qj96w-JMiYFw"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Start the training and testing dataset pipeline. First, define the\n",
        "basic_english tokenizer, which splits the text on spaces (that is, word\n",
        "tokenization):"
      ],
      "metadata": {
        "id": "UCvpNIUijEAP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchtext.data.utils import get_tokenizer\n",
        "\n",
        "tokenizer = get_tokenizer('basic_english')"
      ],
      "metadata": {
        "id": "EiBjmv0pjHjq"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " Use the tokenizer to build the token vocabulary:"
      ],
      "metadata": {
        "id": "tyMeqtVvjLRg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchtext.datasets import IMDB\n",
        "\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "\n",
        "\n",
        "def yield_tokens(data_iter):\n",
        "    for _, text in data_iter:\n",
        "        yield tokenizer(text)\n",
        "\n",
        "\n",
        "vocabulary = build_vocab_from_iterator(\n",
        "    yield_tokens(IMDB(split='train')),\n",
        "    specials=[\"<unk>\"])\n",
        "vocabulary.set_default_index(vocabulary[\"<unk>\"])"
      ],
      "metadata": {
        "id": "G8zGQZN0jO4g"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " Define the `collate_batch` function, which takes a batch of tokenized samples with varying lengths, and concatenates them in a single long sequence of tokens:"
      ],
      "metadata": {
        "id": "V4x7fVFOj8dV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def collate_batch(batch):\n",
        "    labels, samples, offsets = [], [], [0]\n",
        "    for (_label, _sample) in batch:\n",
        "        labels.append(int(_label) - 1)\n",
        "        processed_text = torch.tensor(\n",
        "            vocabulary(tokenizer(_sample)),\n",
        "            dtype=torch.int64)\n",
        "        samples.append(processed_text)\n",
        "        offsets.append(processed_text.size(0))\n",
        "    labels = torch.tensor(\n",
        "        labels,\n",
        "        dtype=torch.int64)\n",
        "    offsets = torch.tensor(\n",
        "        offsets[:-1]).cumsum(dim=0)\n",
        "    samples = torch.cat(samples)\n",
        "\n",
        "    return labels, samples, offsets"
      ],
      "metadata": {
        "id": "1zPG-ecVkCIN"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define the LSTM model:"
      ],
      "metadata": {
        "id": "ZeL0kweXkGiV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LSTMModel(torch.nn.Module):\n",
        "    def __init__(self,\n",
        "                 vocab_size,\n",
        "                 embedding_size,\n",
        "                 hidden_size,\n",
        "                 num_classes):\n",
        "        super().__init__()\n",
        "\n",
        "        # Embedding field\n",
        "        self.embedding = torch.nn.EmbeddingBag(\n",
        "            num_embeddings=vocab_size,\n",
        "            embedding_dim=embedding_size)\n",
        "\n",
        "        # LSTM cell\n",
        "        self.rnn = torch.nn.LSTM(\n",
        "            input_size=embedding_size,\n",
        "            hidden_size=hidden_size)\n",
        "\n",
        "        # Fully connected output\n",
        "        self.fc = torch.nn.Linear(\n",
        "            hidden_size, num_classes)\n",
        "\n",
        "    def forward(self, text_sequence, offsets):\n",
        "        # Extract embedding vectors\n",
        "        embeddings = self.embedding(\n",
        "            text_sequence, offsets)\n",
        "\n",
        "        h_t, c_t = self.rnn(embeddings)\n",
        "\n",
        "        return self.fc(h_t)"
      ],
      "metadata": {
        "id": "sFpVMn64kKQm"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define `train_model(model, cost_function, optimizer,\n",
        "data_loader)` and `test_model(model, cost_function,\n",
        "data_loader)` functions:"
      ],
      "metadata": {
        "id": "aNsGK32WkNQ-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, cost_function, optimizer, data_loader):\n",
        "    # send the model to the GPU\n",
        "    model.to(device)\n",
        "\n",
        "    # set model to training mode\n",
        "    model.train()\n",
        "\n",
        "    current_loss = 0.0\n",
        "    current_acc = 0\n",
        "\n",
        "    # iterate over the training data\n",
        "    for i, (labels, inputs, offsets) in enumerate(data_loader):\n",
        "        # send the input/labels to the GPU\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "        offsets = offsets.to(device)\n",
        "\n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        with torch.set_grad_enabled(True):\n",
        "            # forward\n",
        "            outputs = model(inputs, offsets)\n",
        "            _, predictions = torch.max(outputs, 1)\n",
        "            loss = cost_function(outputs, labels)\n",
        "\n",
        "            # backward\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        # statistics\n",
        "        current_loss += loss.item() * labels.size(0)\n",
        "        current_acc += torch.sum(predictions == labels.data)\n",
        "\n",
        "    total_loss = current_loss / len(data_loader.dataset)\n",
        "    total_acc = current_acc.double() / len(data_loader.dataset)\n",
        "\n",
        "    print('Train Loss: {:.4f}; Accuracy: {:.4f}'.format(total_loss, total_acc))\n",
        "\n",
        "\n",
        "def test_model(model, cost_function, data_loader):\n",
        "    # send the model to the GPU\n",
        "    model.to(device)\n",
        "\n",
        "    # set model in evaluation mode\n",
        "    model.eval()\n",
        "\n",
        "    current_loss = 0.0\n",
        "    current_acc = 0\n",
        "\n",
        "    # iterate over  the validation data\n",
        "    for i, (labels, inputs, offsets) in enumerate(data_loader):\n",
        "        # send the input/labels to the GPU\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "        offsets = offsets.to(device)\n",
        "\n",
        "        # forward\n",
        "        with torch.set_grad_enabled(False):\n",
        "            outputs = model(inputs, offsets)\n",
        "            _, predictions = torch.max(outputs, 1)\n",
        "            loss = cost_function(outputs, labels)\n",
        "\n",
        "        # statistics\n",
        "        current_loss += loss.item() * labels.size(0)\n",
        "        current_acc += torch.sum(predictions == labels.data)\n",
        "\n",
        "    total_loss = current_loss / len(data_loader.dataset)\n",
        "    total_acc = current_acc.double() / len(data_loader.dataset)\n",
        "\n",
        "    print('Test Loss: {:.4f}; Accuracy: {:.4f}'.format(total_loss, total_acc))\n",
        "\n",
        "    return total_loss, total_acc"
      ],
      "metadata": {
        "id": "k5QNPQHpkTqv"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Proceed with the experiment. Instantiate the LSTM model, the cross-entropy cost function, and the Adam optimizer:"
      ],
      "metadata": {
        "id": "drZ-xRMkkovh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = LSTMModel(\n",
        "    vocab_size=len(vocabulary),\n",
        "    embedding_size=64,\n",
        "    hidden_size=64,\n",
        "    num_classes=2)\n",
        "\n",
        "cost_fn = torch.nn.CrossEntropyLoss()\n",
        "optim = torch.optim.Adam(model.parameters())"
      ],
      "metadata": {
        "id": "vc5zEvW-ktYJ"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define `train_dataloader`, `test_dataloader`, and their respective\n",
        "datasets (use mini-batch size of 64):"
      ],
      "metadata": {
        "id": "eKC6gOlakxie"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchtext.data.functional import to_map_style_dataset\n",
        "\n",
        "train_iter, test_iter = IMDB()\n",
        "train_dataset = to_map_style_dataset(train_iter)\n",
        "test_dataset = to_map_style_dataset(test_iter)\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "train_dataloader = DataLoader(\n",
        "    train_dataset, batch_size=64,\n",
        "    shuffle=True, collate_fn=collate_batch)\n",
        "test_dataloader = DataLoader(\n",
        "    test_dataset, batch_size=64,\n",
        "    shuffle=True, collate_fn=collate_batch)"
      ],
      "metadata": {
        "id": "QGYu4No1k4oi"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run the training for 5 epochs:"
      ],
      "metadata": {
        "id": "FaL1sm_4lHxE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(5):\n",
        "    print(f'Epoch: {epoch + 1}')\n",
        "    train_model(model, cost_fn, optim, train_dataloader)\n",
        "    test_model(model, cost_fn, test_dataloader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GIIn5G6ylJ6m",
        "outputId": "bcb81606-c9e4-489c-d0f8-3ab0e7811c80"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1\n",
            "Train Loss: 0.5504; Accuracy: 0.7045\n",
            "Test Loss: 0.4375; Accuracy: 0.7950\n",
            "Epoch: 2\n",
            "Train Loss: 0.3090; Accuracy: 0.8740\n",
            "Test Loss: 0.3390; Accuracy: 0.8559\n",
            "Epoch: 3\n",
            "Train Loss: 0.2240; Accuracy: 0.9135\n",
            "Test Loss: 0.3137; Accuracy: 0.8717\n",
            "Epoch: 4\n",
            "Train Loss: 0.1692; Accuracy: 0.9393\n",
            "Test Loss: 0.3277; Accuracy: 0.8699\n",
            "Epoch: 5\n",
            "Train Loss: 0.1245; Accuracy: 0.9586\n",
            "Test Loss: 0.3427; Accuracy: 0.8717\n"
          ]
        }
      ]
    }
  ]
}